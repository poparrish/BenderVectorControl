import cv2
import numpy as np
import math
from operator import itemgetter
from PIL import Image
import PIL.ImageOps
import rospy
from std_msgs.msg import String
from buildTarget import getTarget
from calc_pose import calculate_translation
"""
author: parker
this is a proof-of-concept script that identifies a potential "landing platform" and  uses perspective transforms to calculate
the pose (position/orientation). Creates a rostopic and node so we can integrate with the simulation.
"""

def callback(x):
    #the cv2.createTrackbar() requires callback param
    pass

#TODO
def calibrate_gimbal():
    """
    come up with a way to run a calibration sequence at startup. For now just set Yaw and Pitch to global variables
    :return: returns nothing
    """
    pass
#TODO
def get_current_gimbal():
    """
    :return: should just return the current pitch&yaw of the gimbal.
    """
    pass
#TODO
def update_gimbal_pid(servoPitch_pos, servoYaw_pos, bounded_rect_coords):
    """
    :param servoPitch_pos: servos current Pitch. Use global variables to update position
    :param servoYaw_pos: servos current Yaw. Use global variables to update position
    :param bounded_rect_coords: use bounded_rect_coords instead of sorted_beacons so that we can update gimbal with less than 4 beacons.
    :return: returns nothing
    """
    pass


def filter_rectangles(sort_rect, num_rect):
    """
    Right now this method just sorts rectangles by area. Assuming the largest rectangels are markers on the target.
    :param sort_rect: [[xstart,ystart,xlength,ylength,area]] 2D list of all rectangles found
    :param num_rect: int. how many rectangles should be returned
    :return: [[xstart,ystart,xlength,ylength,area]] 2D list of the top num_rect rectangles
    """
    sorted_area = sorted(sort_rect, key=itemgetter(4))
    sorted_area.reverse()
    if len(sorted_area) > num_rect:
        for i in sorted_area:
            sorted_area = sorted_area[:-1]
            if len(sorted_area) == num_rect:
                break
    return_rectangles = sorted_area
    return return_rectangles

def convert_to_cartesian(beacons):
    """
    currently unused.
    :param beacons:
    :return:
    """
    i = 0
    # print 'default to cartesian in'
    # print beacons
    for beacon in beacons:
        #convert x
        if beacon[0] > WIDTH/2:
            beacon[0] = beacon[0]-WIDTH/2
        elif beacon[0] < WIDTH/2:
            beacon[0] = WIDTH/2-beacon[0]
            beacon[0] *= -1
        else:
            beacon[0] = 0
        # print 'beacon val'
        # print beacon[0]
        beacons[i][0] = beacon[0]
        # print 'array val'
        # print beacons[i][0]
        #convert y
        if beacon[1] > HEIGHT/2:
            beacon[1] = beacon[1]-HEIGHT/2
            beacon[1] *= -1
        elif beacon[1] < HEIGHT/2:
            beacon[1] = HEIGHT/2-beacon[1]
        else:
            beacon[1] = 0
        beacons[i][1] = beacon[1]
        i+=1
    # print 'default to cartesian out'
    # print beacons
    return beacons

def cartesian_to_default(sorted_beacons):
    """
    currently unused
    :param sorted_beacons:
    :return:
    """
    for beacon in sorted_beacons:
        #convert x
        if beacon[0] > 0:
            beacon[0] += WIDTH/2
        elif beacon[0] < 0:
            beacon[0] *= -1
            beacon[0] = WIDTH/2 - beacon[0]
        else:
            beacon[0] = WIDTH/2
        #convert y
        if beacon[1] > 0:
            beacon[1] = HEIGHT/2 - beacon[1]
        elif beacon[1] < 0:
            beacon[1] *= -1
            beacon[1] = HEIGHT/2 + beacon[1]
        else:
            beacon[1] = HEIGHT
    # print 'cartesian_to_default out'
    # print sorted_beacons
    printable_coords = sorted_beacons
    return printable_coords

def get_bounded_rect_coords(sorted_beacons):
    """
    finds the xy coordinates of the top num_rect rectangles. where xy is the center point of the rectangle
    :param sorted_beacons: top num_rect beacons we want to find coords for
    :return: [[xcoord,ycoord]] 2D list
    """
    for coords in sorted_beacons:
        xcoord = coords[0] + coords[2]/2
        ycoord = coords[1] + coords[3]/2
        coords[0] = xcoord
        coords[1] = ycoord
    bounded_rect_cords = sorted_beacons
    return bounded_rect_cords

def sort_beacons(cartesian_coords):
    """
    currently unused
    :param cartesian_coords:
    :return:
    """
    sorted_by_y = sorted(cartesian_coords, key=itemgetter(1))
    sorted_by_y.reverse()#smallest to largest
    try:
        #now we are now smallest to largest y, swap slots 0-1 and 2-3 if necessary to get x in order
        if(sorted_by_y[0][0] < sorted_by_y[1][0]):#if first x is smaller than second x
            sorted_by_y[0], sorted_by_y[1] = sorted_by_y[1], sorted_by_y[0]
        if(sorted_by_y[2][0] > sorted_by_y[3][0]):#if first x is smaller than second x
            sorted_by_y[2], sorted_by_y[3] = sorted_by_y[3], sorted_by_y[2]
    except IndexError:
        sorted_by_y = [0,0,0,0,0]
    return sorted_by_y

def distance_between(coord1, coord2):
    """
    currently unused
    :param coord1:
    :param coord2:
    :return:
    """
    try:
        difference = math.sqrt((math.fabs((coord1[0]-coord2[0])*(coord1[0]-coord2[0]))) + math.fabs((coord1[1]-coord2[1])*(coord1[1]-coord2[1])))
    except TypeError or ValueError:
        difference = 0
    return difference

def slice_coords(beacon):
    return beacon[0:2]

def calcPose():
    """
    Finds the pose of the drone relative to the target.
    first it finds the coordinates of all the beacons
    orders them using buildTarget() so that it knows which beacon is which for the pose transform
    performs the pose transform provided it has all 7 points from the hull
    modifications to the output may be necessary
    :return:[xtheta,ytheta,ztheta,xoffset,yoffset,zdistance]
    """
    # grab the frame
    ret, frame = cap.read()
    original = frame

    # get trackbar positions
    ilowH = cv2.getTrackbarPos('lowH', 'image')
    ihighH = cv2.getTrackbarPos('highH', 'image')
    ilowS = cv2.getTrackbarPos('lowS', 'image')
    ihighS = cv2.getTrackbarPos('highS', 'image')
    ilowV = cv2.getTrackbarPos('lowV', 'image')
    ihighV = cv2.getTrackbarPos('highV', 'image')

    # filter out the hsv values for the "beacons" we want and create bounding rectangles
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    lower_hsv = np.array([ilowH, ilowS, ilowV])
    higher_hsv = np.array([ihighH, ihighS, ihighV])
    mask = cv2.inRange(frame, lower_hsv, higher_hsv)
    blank_slate = cv2.inRange(frame, 0, 0)
    _, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
    num_contour = 0
    sort_rect = []
    for c in contours:
        if cv2.contourArea(c) < 10:
            continue
        rect = cv2.boundingRect(c)
        x, y, w, h = rect
        cv2.rectangle(original, (x, y), (x + w, y + h), (0, 255, 0), 2)
        num_contour += 1
        data = [x, y, w, h, w * h]
        sort_rect.append(data)

    # filters all detected rectangles for only the num_rect largest
    num_rect = 7
    beacons = filter_rectangles(sort_rect, num_rect)
    bounded_rect_cords = get_bounded_rect_coords(beacons)

    sliced = []
    for _ in bounded_rect_cords:
        sliced.append(_[:2])
    print "sliced: ", sliced

    if len(sliced) != 7:
        print 'no image lock'
    else:
        # target_size = getTarget(bounded_rect_cords)
        try:
            # print len(size)
            target = getTarget(bounded_rect_cords)
            # to perform the cv2.PnP transform OpenCV needs the default coordinate settings so we revert the sorted beacons back to default coordinate system
            # target = cartesian_to_default(sorted_beacons)

            # Camera settings these basically assume a perfect projection plane and will need to be calibrated. just fillers for now
            focal_length = 510
            camera_matrix = np.array([
                (focal_length, 0, WIDTH / 2),
                (0, focal_length, HEIGHT / 2),
                (0, 0, 1)
            ], dtype="double")


            # 2D points (use target)
            print "target: ", target
            target_points = np.array([
                (target[0][0], target[0][1]),
                (target[1][0], target[1][1]),
                (target[2][0], target[2][1]),
                (target[3][0], target[3][1]),
                (target[4][0], target[4][1]),
                (target[5][0], target[5][1]),
                (target[6][0], target[6][1])
            ], dtype="double")

            # ur balz
            # 3D points (arbitrary reference frame. Measured dimensions in cm with upper middle as origin
            model_points = np.array([
                (-45.7, 0, 86.3),
                (-48.2, 0, 41.3),
                (-45.7, 0, 2.5),
                (0, 0, 0),
                (45.7, 0, 2.5),
                (48.2, 0, 41.3),
                (45.7, 0, 86.3)
            ], dtype="double")

            distortion_coeffs = np.zeros(
                (5, 1), dtype="double"
            )
            # assume zero distortion...this is practically the case
            # distortion_coeffs =  np.array([#got these from running calibration.py
            #     (.13006041, .16734663,.00304074,-.01473097,-1.71723664)
            # ], dtype="double")

            # Perspective transform allows us to extract the rotation_vector and translation_vector
            # note that the translation vector is from the center of the cameras frame to the camera at the same depth as the 0,0,0 beacon.

            (success, rotation_vector,
             translation_vector) = cv2.solvePnP(model_points, target_points, camera_matrix, distortion_coeffs)

            # adds the bgr axes to the original image because fancy
            (zAxis, jacobian) = cv2.projectPoints(
                np.array([(0.0, 0.0, 30.0)]), rotation_vector, translation_vector, camera_matrix, distortion_coeffs
            )
            (yAxis, jacobian) = cv2.projectPoints(
                np.array([(0.0, 30.0, 0.0)]), rotation_vector, translation_vector, camera_matrix, distortion_coeffs
            )
            (xAxis, jacobian) = cv2.projectPoints(
                np.array([(30.0, 0.0, 0.0)]), rotation_vector, translation_vector, camera_matrix, distortion_coeffs
            )
            p1z = (int(target_points[3][0]), int(target_points[3][1]))
            p2z = (int(zAxis[0][0][0]), int(zAxis[0][0][1]))
            cv2.line(original, p1z, p2z, (255, 0, 0), 2)  # (blue)
            p1y = (int(target_points[3][0]), int(target_points[3][1]))
            p2y = (int(yAxis[0][0][0]), int(yAxis[0][0][1]))
            cv2.line(original, p1y, p2y, (0, 255, 0), 2)  # (green)
            p1x = (int(target_points[3][0]), int(target_points[3][1]))
            p2x = (int(xAxis[0][0][0]), int(xAxis[0][0][1]))
            cv2.line(original, p1x, p2x, (0, 0, 255), 2)  # (red)

            # adjust rotation_matrix
            camera_angle = .486
            rotation_vector[0] -= np.pi * -1  # flip x to correct orientation
            # rotation_vector[2] *= -1 #flip our z axis so that positive is facing the camera

            x_theta = -camera_angle  # add gimbals reported camera angle to x
            z_theta = 0  # rotation_vector[1]
            y_theta = 0  # rotation_vector[2]

            a = translation_vector[0]
            b = translation_vector[1]
            c = translation_vector[2]

            # print "no rotation" , (a, b, c)
            a1 = a * math.cos(z_theta) - b * math.sin(z_theta)
            b1 = a * math.sin(z_theta) + b * math.cos(z_theta)
            c1 = c
            # print "x axis rotation", (a1, b1, c1)

            c2 = c1 * math.cos(y_theta) - a1 * math.sin(y_theta)
            a2 = c1 * math.sin(y_theta) + a1 * math.cos(y_theta)
            b2 = b1
            # print "y axis rotation", (a2, b2, c2)

            b3 = b2 * math.cos(x_theta) - c2 * math.sin(x_theta)
            c3 = b2 * math.sin(x_theta) + c2 * math.cos(x_theta)
            a3 = a2

            a3 = (math.atan(rotation_vector[2]) * c3) - translation_vector[0]

            # print "z axis rotation", (a3, b3, c3)

            # translation = calculate_translation(rotation_vector[2], translation_vector[2], APPROACH_DIST_CM)
            # print "translation = ", translation
            # print "(a3, b3, c3) = ", (a3, b3, c3)
            print "rotation_vector", format(np.rad2deg(rotation_vector))  # radians (x,y,z)
            print "translation_vector", format(
                translation_vector
            )  # calibrated to be cm. this was done in the camera calibration matrix
            pose = []
            for i in range(3):
                pose.append(translation_vector[i])
            for i in range(3):
                pose.append(np.rad2deg(rotation_vector[i]))
            # print "pose",pose[3]
        except:
            print 'couldnt reconstruct target'

    return pose



def start():
    """
    creates a topic called Pose_Estimation under the Pose node.
    Publishes the output of calcPose() as a string separated by ',' its lower bandwidth over ROS that way.
    :return: nothing
    """

    # ROS stuff
    pub = rospy.Publisher('Pose_Estimation', String, queue_size=1)
    rospy.init_node('Pose')
    r = rospy.Rate(10)#<-refreshrate
    while not rospy.is_shutdown():
        try:
            pub.publish()
            pose = calcPose()
            print "pose:",pose
            returnString = str(str(int(pose[0])) + ',' + str(int(pose[1])) + ',' + str(int(pose[2])) + ',' + str(int(pose[3])) + ',' + str(int(pose[4])) + ',' + str(int(pose[5])))
            #print returnString
            pub.publish(returnString)
        except:
            returnString = str(str(int(0)) + ',' + str(int(0)) + ',' + str(int(0)) + ',' + str(int(0)) + ',' + str(int(0)) + ',' + str(int(0)))
            #print returnString
            pub.publish(returnString)
        r.sleep()


    rospy.spin()


if __name__ == '__main__':

    #these are just for fixed position camera. we will have to plug in what the gimbal reports for CAM_ANGLE rather
    #than calculating it by hand here
    CAM_HEIGHT = 80
    CAM_DIST = 160
    CAM_ANGLE = 90 - (math.atan(CAM_DIST / CAM_HEIGHT) * 100)

    #set up the camera frame
    print 'camangle'
    print CAM_ANGLE
    cap = cv2.VideoCapture(1)
    cv2.namedWindow('image')
    WIDTH = 640
    HEIGHT = 480
    cap.set(3, WIDTH)  # width
    cap.set(4, HEIGHT)  # height

    #configure the HSV that will be used to find beacons
    ilowH = 0
    ihighH = 126
    ilowS = 0
    ihighS = 124
    ilowV = 143
    ihighV = 255
    # create trackbars for color change
    cv2.createTrackbar('lowH', 'image', ilowH, 255, callback)
    cv2.createTrackbar('highH', 'image', ihighH, 255, callback)
    cv2.createTrackbar('lowS', 'image', ilowS, 255, callback)
    cv2.createTrackbar('highS', 'image', ihighS, 255, callback)
    cv2.createTrackbar('lowV', 'image', ilowV, 255, callback)
    cv2.createTrackbar('highV', 'image', ihighV, 255, callback)

    #APPROACH_DIST_CM = 250

    start()



